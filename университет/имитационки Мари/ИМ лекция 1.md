# ака Базовые принципы метода Монте Карло
```
Если так и дальше пойдет, то я под конец семестра напьюсь за чей-то счет. Земля нам всем пуховиком
```
# ЧЗХ???
Вот есть у нас такая ситуевина: нужно решить некоторую задачку, но в лоб решать долго, дорого, не хочется, невозможно, лень и прочие отговорки не учить этот предмет. Поэтому математики взяли и придумали *гениальнейшее* решение - решать с помощью рандома, имитируя 100500 экспериментов на бумаге. RNGsus получил новую базу последователей с введением этой дисциплины в курс.

Как мне объяснил ЧатЖПТ, сути там фиг да нифига:
- Смоделировать задачу
- Ввести случайный компонент
- Провести вычисления
- Агрегировать эти вычисления (да прибудет с нами статистика)

## Пример с подбрасыванием монеты
Допустим нам жесть как не хочется кидать монету 100500 раз, а узнать, ровная она или не совсем, хочется. 

```
Подскажите кто-нибудь эквиваленты fair coin и weighted coin на великом и могучем......
```

$\mathfrak{U}(0,1)$ - равномерное распределение от 0 до 1. Кто забыл, гляньте мой конспект с 1го тервера

$$
P(x) = \begin{cases}
0, \quad x \lt 0 \\
1, \quad 0 \leq x \leq 1 \\
0, \quad x \gt 1
\end{cases}
$$

$$
F(x) = \begin{cases}
0, \quad x \lt 0 \\
x, \quad 0 \leq x \leq 1 \\
1, \quad x \gt 1
\end{cases}
$$

```
Почему мы берем НСВ, когда исходов всего 2? Кто объяснит, тому с меня кофе
```

> [!К чему это было? я просто оставлю это здесь]
> Алгоритм вихря Мерсенна (Mersenne Twister) - ГПСЧ средней паршивости (по крайней мере для криптографии), основанный на простых числах Мерсенна - $2^n-1$, где $n$ - простое число. Период такой штуки $2^{19337} - 1$. Возможно поэтому он называется MT19337

```
Не очень важная пометка - сами по себе числа Мерсенна далеко не все простые, но если число простое, то и n тоже простое. Обратное неверно, контрпример - n=11
```

Так, распределение ввели, осталось из него сделать выборку

$$\{\mathfrak{U}_i\}_{i=1}^n$$

Из них допустим выпало $m$ гербов. Получаем, что

$$
\frac{m}{n}\ \ \underset{n \rightarrow \infty}{\overset{p}{\longrightarrow}}\ \ p
$$

... эта дробь сходится по вероятности к числу $p$, в нашем случае $\frac{1}{2}$

#### Лирическое отступление
==Вспомним неравенство Чебышева==

$$
\forall \varepsilon > 0 \quad \mathbb{P}(\ |\xi - M_\xi| \geq \varepsilon\ ) \leq \frac{D_\xi}{\varepsilon^2}
$$
^^Это из тервера^^
Ну тут вроде бы все понятно. Вероятность того, что при случайном тыке мы *не* попадем в эпсилон от мат.ожидания не выходит за дробь в правой части 

Проще наверное осознать обратное (следствие из н.Ч.) - $\mathbb{P}$ попадания в эпсилон $\geq 1-\frac{D_\xi}{\varepsilon^2}$

$$
\mathbb{P}(\ |\frac{m}{n} - p\ | \leq \varepsilon\ ) \geq 1 - \frac{p\rho}{n\varepsilon^2}
$$
^^А это то, что было записано преподом^^ Какого черта там делает $\frac{p\rho}{n}$, я так и не поняла. Наверное оно равняется дисперсии, хотя я уже ничему не верю. И что такое ро.......

В общем и целом - суть этого сегмента показать, что через ЗБЧ имеем сходимость среднего с.в. к её мат.ожиданию (при условии конечных $M_\xi$ и $D_\xi$) по вероятности

А также из неведомо как полученной формулировки препода мы видим зависимость точности от объема выборки. Чтобы повысить точность в 10 раз, надо в 100 раз увеличить выборку и что-то там куда-то там. И всё из-за квадрата эпсилона в знаменателе, только в новой формуле там ещё и объём выборки

**Лирическое отступление окончено**

## Оценка числа пи
А вот этот пример имхо гораздо понятнее, чем с монетой

![[Pasted image 20250204141235.png]]

сгенерим равномерно много много точек $\{(x_i, y_i)\}_{i=1...n}$ и посчитаем сколько попало внутрь вписанной окружности (обзовем этот феномен событием $A$). По геометрической вероятности:
$$\mathbb{P} = \frac{S_{c}}{S_{sq}} = \frac{\pi}{4}$$

Это с одной стороны, где есть оцениваемое пи. А с другой имеем $I(A)$ - индикатор попадания внутрь окружности (1 - попали, 0 - не попали). Каждой точке $(x_i, y_i)$ соответствует $I_i(A)$:

$$
\mathbb{P}(x^2+y^2 \leq 1) = \mathbb{E}\ I(A) ≈ \frac{1}{n} \sum_{i=1}^n I_i(A)
$$

Вот для этого примерно равенства и нужны были неравенство Чебышева и ЗБЧ. Поверим наслово и поедем дальше.

# Общая постановка задачи

> [!Цель этой дряни]
> Дать оценку некоторому $a = \mathbb{E}X$ (по-Роговски $M_\xi$) - мат.ожиданию с.в. с пока что известной дисперсией $\sigma^2$

```
Я бы эту а как-нибудь по-другому обозвала, шобы оно не терялось среди прочих гораздо более красивых буков. Но i suppose в этом есть свой шарм
```

Сгенерим выборку и посчитаем выборочное среднее:

$$
\overline{x} = \frac{1}{n} \sum_{i=1}^n x_i
$$

Если внезапно дисперсия неизвестна (что по моему предположению довольно частый случай), то её оцениваем так:

$$
\sigma^2 ≈ \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \overline{x})^2
$$

Доверительный интервал такой оценки:

$$
\mathbb{P}\ \left( \overline{x} - \frac{z_\gamma\sigma}{\sqrt{n}} < a < \overline{x} + \frac{z_\gamma\sigma}{\sqrt{n}} \right) ≈ \gamma
$$

где $z_\gamma = \Phi^{-1}(\frac{\gamma}{2})$.

Напоминаю, что:
$$
\Phi(x) = \frac{1}{\sqrt{2\pi}} \cdot \int_{0}^{x} e^{-\frac{\tau^2}{2}} d\tau
$$

\- функция Лапласа ака интеграл от плотности стандартизованного нормального распределения, берущийся в квадратуру, но все же записывающийся в виде *буквы*, а не выражения.

Вспоминая курс лабораторного анализа от себя добавлю, что доверительный интервал это тупо отсечка крайних значений. Мы как будто бы говорим, что с самого края лежат помехи и промахи и мы им буквально не доверям. А вся голимотья выше это вероятность того, что какое-то конкретное значение выборки войдет в итоговые вычисления. Как это выразить в формулах, я понятия не имею, нам тупо выдали ехельку, в которую были вшиты все эти вычисления и она сама строила и последовательности, и интерполяцию, и регрессии. 

И нет, этот файл я вам не дам не потому что я жучара та еще, а потому что он сгинул в небытие. Это было классе в 8м, с тех пор много воды утекло в мои проекты и курсачи.

Гамма берется обычно либо 0.9, 0.99 или 0.999. Найду тетрадь с лаб. анализом, подсмотрю больше инфы.

Мой совет - не пытайтесь осознать $z_\gamma = \Phi^{-1}(\frac{\gamma}{2})$ как есть. Удобнее обратно обратить Фи и понять, что:

$$\frac{\gamma}{2} = \Phi(z_\gamma) \ \Rightarrow\ \gamma = 2\cdot\Phi(z_\gamma)$$

А это буквально...

![[Pasted image 20250205112338.png]]

... вот эта вот площадь. То есть учитываются все значения, которые мы берем в рассмотрение, исключая крайности

$z_\gamma$ - точность (половина доверительного интервала (?) - из картинки возможно понятно почему)

$$
\varepsilon = \frac{z_\gamma\sigma}{\sqrt{n}}
$$

Вышеизложенный эпсилон это верхняя граница ошибки с доверительной вероятностью $\gamma$

Из этого можно выразить объем выборки, обеспечивающий необходимую точность $\varepsilon$ с надежностью $\gamma$:

$$
n = \frac{\sigma^2 z^2_\gamma}{\varepsilon^2}
$$

Ышо один момент - наличие дисперсии $\sigma^2$ в числителе. чем больше дисперсия, тем выше требуемый объем выборки

> ?????????

Это всё, что у меня написано далее. Остальное осознавалось мной вне лекций 
# Распределение ДСВ
Пусть задана ДСВ:

$$
X: \{x_j, p_j\}; \quad \mathbb{P}(X = x_j) = p_j \quad j = 1,...,m
$$

Берем отрезок от 0 до 1 и разбиваем его на $m$ полуинтервалов следующего вида:

$$
\Delta_1 = [0, p_1) \quad \Delta_2 = [p_1, p_1+p_2) \quad ...
$$
$$
\Delta_m = [\sum_{i=1}^{m-1}p_i, 1]
$$

И сгенерим $n$ значений *другой* с.в., например, $U$. Если $u_i \in \Delta_j$ (вторая с.в. попала в заданные промежутки), то за $i$-е значение выборки берется $x_j$ (замудреный способ сказать, что всё, что попало в промежутки, слепляется в исключенную верхнюю границу)

```
Авторские рассуждения: зачем тогда лепить исключение верхней границы, если мы по сути сплющиваем в неё промежуток. Не было бы логичнее наоборот исключить нижнюю и сказать, что граница принадлежит полуинтервалу? Да и если придираться, то полуинтервалы здесь все кроме одного - последнего.
```

# Метод обратной функции
## Как это было на лекции
> [!Дано:]
> $F_{X}(x)$ - функция распределения с.в. $X$
> Обозначим $Y = F_X^{-1}(U)$, где $U$ - НСВ, равномерно распределенная от 0 до 1

Дальше идет то, что осознаётся достаточно непросто, но я постараюсь

$$
\begin{aligned}
& F_Y(x) = \mathbb{P}(Y<x) = \mathbb{P}(F_X^{-1}(U) < x) = \\ & = \mathbb{P}(U < F_X(x)) = F_X(x)
\end{aligned}
$$

==Какой вывод мы делаем из этой выкладки?==

Из 1 части тервера мы ~~помним~~ вспомнили, что функция от с.в. тоже сама по себе является с.в., а значит имеет плотность и распределение так же как и любая другая с.в.

В Роговских понятиях это бы выглядело так:
$$
\begin{aligned}
& \xi \rightarrow X \\
& \upsilon \rightarrow U \\ 
& \eta = F_\xi^{-1}(\upsilon)
\end{aligned}
$$

$$
\begin{aligned}
& F_\eta(x) = \mathbb{P}(\eta < x) = \mathbb{P}(F_\xi^{-1}(\upsilon) < x) = \\ & \mathbb{P}(\upsilon < F_\xi(x)) = F_\upsilon(F_\xi(x))
\end{aligned}
$$

Получили равномерное распределение реузльтирующей с.в. А поскольку равномерное распределение задается следующей функцией...

$$
F_\upsilon(x) = \begin{cases}
0, & x<0 \\
x, & 0 \leq x \leq 1 \\
1, & x > 1
\end{cases}
$$

... и нас интересует только промежуток от 0 до 1 (см. условие задачи), то можно вместо $F_\upsilon(F_\xi)$ оставить просто её аргумент - $F_\xi$

==Логично? Да вроде логично. Понятно? Да нихрена. Ща поясню==

## Как это понять на пальцах без формул краткое содержание читать онлайн бесплатно

$\mathfrak{U}(0,1)$ - это выбор числа от 0 до 1 включая 0 и включая 1. Можем сказать, что мы случайно выбираем вероятность. По свойствам $F_\xi(x)$ она неубывает и непрерывна слева, следовательно её область значений *гарантированно* содержит весь промежуток от 0 до 1 включительно. А поскольку это *функция* распределения, то каждому $F_\xi(x)$  соответствует один и только один $x$. Значит если мы тыкнем пальцем в $[0,1]$, то мы гарантированно найдем такой $x = F_\xi^{-1}(u)$, который выдаст нужную вероятность. И чем больше угол наклона касательной (он же производная, он же плотность распределения), тем с бОльшей вероятностью мы тыкнем в соответствующий $x$. 

> Иначе говоря, так можно отобразить $\mathfrak{U}(0,1)$ на *любое* другое распределение.

```
Можно, но только в теории. На практике обратные функции обычно занимают очень много вычислений и бывают весьма не точными. Поэтому мы и раскапываем более приемлемые преобразования, например метод Марсальи
```

![[Pasted image 20250208011357.png]]

Я сделала [пример](https://www.desmos.com/calculator/hqwe5kdcsk) для таких отбитых визуалов как я, которым без картинки тупо не понять как бы тщательно не разжевывали. Тут равномерное распределение (горизонтальные линии) положено на нормальное (вертикальные линии). Оси жмыхнутые для удобства восприятия.

_Поправочка на ветер: я победила десмос и теперь там можно подвигать равномерное распределение. Правда работает эта дрянь крайне медленно и костыльно, но она работает и даже типа интерактивно. кушаем и наслаждаемся_

# Моделирование некоторых непрерывных распределений на основе ГПСЧ

Тут какая-то табличка без особых пояснений, и в Ксюшиных записях особо ничего внятного нет, поэтому я просто оставлю это здесь

![[Pasted image 20250205143503.png]]

Наверное тут говорится о том, что можно подменить теоретические случайные ряды практическими псевдослучайными и какие буквы туда подставлять чтобы получить правильные другие буквы?

# Моделирование нормального распределения

Пусть $X$ распределена по $\mathbb{N}(0,1)$. Тогда имеем:

$$
F_X(x) = \frac{1}{2} + \Phi(x)
$$

А также пусть мы имеем $U_1, U_2, ..., U_k$ - независимые равномерно распределенные на $[0, 1]$ с.в.

Центральная предельная теорема гласит (по версии Рогова), что:

$$
\lim_{n\rightarrow \infty} \mathbb{P} \left(\frac{S_n - na}{\sqrt n \sigma} <x\right) = \frac{1}{2} + \Phi(x)
$$

$S_n$ - сумма $n$ одинаковых случайных величин с мат. ожиданием $a$ и СКО $\sigma$

Поскольку у каждой такой с.в. (см. выше) $a =\mathbb{E}\ {U_{i}} = 0.5$ и $\sigma^2 = 1/12$ *(да, я руками считала интегралы, забытый богом и мною матанализ нервно курит в сторонке)*, то подставляя их в формулу, получаем следующую запись из презентации

$$
S_k = \frac{\underset{i=1}{\overset{k}{\sum}} U_i - k/2}{\sqrt{k/12}}
$$

Это то же выражение в скобках с единственной лишь разницей в конечности этой суммы (иначе - допредельной форме). По-простому говоря, если взять достаточно большое $k$, то можно посчитать с некоторой погрешностью, что распределение такой суммы будет очень похожим на стандартизованное нормальное

# Преобразование Бокса-Мюллера
```
эхб...
```
Наступило то время, когда читаешь статьи из википедии и *хотя бы немного но понимаешь чзх там написали умные люди умными буквами*

Применяется, когда нужно равномерную с.в. переконвертировать в нормальную

### Какая-то теорема
> Пусть имеются $U_1$ и $U_2$ - независимые равномерно распределенные с.в. на $[0,1]$. Тогда
> $$X = \cos(2\pi U_1) \sqrt{-2\ln U_2}$$
> $$Y = \sin(2\pi U_1) \sqrt{-2\ln U_2}$$
> независимы и имеют стандартизованное нормальное распределение

### Доказательство

Несмотря на обещания разобраться позже, эта тема не давал мне покоя последние несколько дней. И вот, доказательство:

Без тени сомнения рассмотрим случайный вектор $(X,Y)$, *имеющий стандартизованное нормальное распределение*. Смелое заявление. Тогда плотность распределения (в декартовых координатах) будет выглядеть так:

$$P_{X,Y}(x,y) = \frac{1}{2\pi}e^{-\frac{1}{2}\cdot(x^2+y^2)}$$

![[Pasted image 20250207131647.png]]

Если мы это построим в пространстве и внимательно посмотрим на получившуюся поверхность, то самые наблюдательные осознают 2 факта:

1. Это та же самая Гауссова кривая (камон, нормальное распределение), только в пространстве. Ось $z$ тут у меня жутко растянута, чтобы было виднее, на самом деле бугор не такой жирный
2. Эта кривая радиально симметрична относительно $z$. 

```
я напрочь забыла как на великом и могучем обозвать rotational symmetry, да простят меня лингвисты
```

Немного разжую 2 пункт - эту поверхность можно получить вращением куска Гауссовой кривой вокруг оси $z$, а это по определению значит, что в любом сечении, содержащем $z$ мы получим одну и ту же кривую. Запомним этот факт и поедем дальше.

Переведем декартовы координаты в полярные (отчасти из-за найденной симметрии) и опять же придержим этот факт до лучших времен:

$$x = r\cos\varphi \quad X = R\cos\Phi$$
$$y = r\sin\varphi \quad Y = R\sin\Phi$$

> Маленькое примечание: $R$ и $\Phi$ - случайные величины, которые откладываются в полярных координатах по "осям" $r$ и $\varphi$ (вообще там ось одна и это радиус, но вы поняли надеюсь)
> Ну то есть $R>0$ и $0<\Phi\leq2\pi$

При этом плотность станет:

$$P_{R,\Phi}(r, \varphi) = P_{X,Y}(r\cos\varphi, r\sin\varphi) \cdot |\ \mathfrak{J}\ | = $$

где $|\ \mathfrak{J}\ | = |\ \frac{\partial(x,y)}{\partial(r,\varphi)}\ | = r$ - определитель матрицы Якоби (якобиан), состоящей из маргинальных плотностей в каком-то там порядке. Кому не влом позаниматься матанализом, проверяйте, что это действительно так.

$$= \frac{1}{2\pi} e^{-r^2/2} \cdot r, \quad r>0,\ \varphi \in [0, 2\pi]$$

Так мы получили 2-мерную плотность в полярных координатах из декартовых. Чтобы из нее получить маргинальные плотности, проинтегрируем по оставшейся переменной:

$$
P_R = \int_{0}^{2\pi} \frac{1}{2\pi} e^{-r^2/2} \cdot r\ d\varphi
$$

![[Pasted image 20250206232144.png]]

$$
P_\Phi = \int_{0}^{\infty} \frac{1}{2\pi} e^{-r^2/2} \cdot r\ dr
$$

![[Pasted image 20250206232412.png]]

*(Проигнорьте пж область определения этих функций, мне было ужасно лень вписывать, что одна задана на $(0,+\infty)$, а вторая на $[0,2\pi]$)*

А вот тут мы вспоминаем про радиальную симметрию исходной поверхности. Поскольку кривая в любом сечении (при любом $\varphi$) одна и та же, то и значение интеграла выйдет одно и то же. Соответственно, $\Phi$ имеет равномерное распределение на $[0,2\pi]$

С радиусом всё не так просто. Тут надо прям взять и проинтегрировать (за меня это сделал десмос). В итоге мы получим плотность распределения $R$ (вот ту странную кривую). Это распределение Пирсона X типа (туда мы сейчас углубляться не будем), отсюда нам нужен только тот факт, что распределения *квадрата* такой с.в. *экспоненциально с параметром $\lambda=1/2$*

![[Pasted image 20250207135441.png]]

*Объяснение этим выкладкам можно найти в 1 тервере в разделе "Функция от НСВ", вывод $F_{f(\xi)}(x)$ и $P_{f(\xi)}(x)$*

Таким образом, мы видим, что $R^2$ распределена экспоненциально по $\exp(1/2)$, а $\Phi$ - по $\mathfrak{U}(0, 2\pi)$. Самое ужасное позади, осталось немного

Теперь нам надо понять, как связать экспоненциальное и равномерное распределение в квадрате радиуса. 

> Вспомним метод обратной функции, а именно тот факт, что $\mathfrak{U}(0,1)$ можно отобразить на любое другое распределение

Отобразим $U_1, U_2 \sim \mathfrak{U}(0,1)$ на $\exp(1/2)$. Для этого найдем $F_{R^2}^{-1}(x)$:

$$
F_{R^2}(x) = 1-e^{-\lambda x} = 1-e^{-x/2} = U_1
$$

$$
1-U_1 = e^{-x/2} \quad \Rightarrow \quad x = -2\ln(1-U_1)
$$

$1-U_1$ это такая же вероятность, как и $U_1$ (напоминаю о равномерном распределении). А значит эта запись равносильна:

$$
x = -2\ln U_1 = R^2 \quad \Rightarrow \quad R = \sqrt{-2\ln U_1}
$$

Теперь разбираемся с $\Phi$. Тут всё ещё более тривиально. Есть равномерное распределение на $[0,1]$, надо сделать $[0,2\pi]$. Умножим отрезок на $2\pi$ и получим желаемое:

$$
\Phi = 2\pi U_2
$$

И на последнем шаге этого маразма мы переводим полярные координаты обратно в декартовые и убеждаемся в том, что действительно мы получаем нормальное распределение, выраженное из равномерного (ЧиТД). Из этого всего напрямую следует и независимость этих с.в., но если спросят почему, я тактично промолчу. Надеюсь не спросят

$$
X = R\cos \Phi = \sqrt{-2\ln U_1} \cdot \cos(2\pi U_2)
$$

$$
Y = R\sin \Phi = \sqrt{-2\ln U_1} \cdot \sin(2\pi U_2)
$$

Сложно? Да вроде не очень. Осознаваемо? Сомнительно. Кому подробнее надо разжевать, кидайте тапочек, попробую пересказать ещё раз

## Метод Марсальи
> Пусть имеются $u_1$ и $u_2$ - независимые равномерно распределенные с.в. на $[0,1]$. *Дежавю? Нет, имитационка*. Тогда мы можем сделать так:
> $$\upsilon_1 = 2u_1-1$$ 
> $$\upsilon_2 = 2u_2-1$$
> $$s = \upsilon_1^2 + \upsilon_2^2$$
> Пропускаем все $s>1$. Если $0<s\leq 1$, то:
> $$x_1 = \upsilon_1\sqrt{\frac{-2\ln s}{s}}$$
> $$x_2 = \upsilon_2\sqrt{\frac{-2\ln s}{s}}$$
> И эти величины будут распределены по $\mathbb{N}(0,1)$

Интересный факт: почему-то и [ру-сегмент вики](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B5%D0%BE%D0%B1%D1%80%D0%B0%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_%D0%91%D0%BE%D0%BA%D1%81%D0%B0_%E2%80%94_%D0%9C%D1%8E%D0%BB%D0%BB%D0%B5%D1%80%D0%B0), и [хабр](https://habr.com/ru/articles/208684/) этот метод объединяют с методом Бокса-Мюллера, в то время как [англовики](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform) их разогнала по отдельным статьям

**Что сказал по этому поводу великий гугл:** оба эти преобразования - это способ программно сделать из равномерного распределения (которое реализовано во многих библиотеках криптографии) стандартизованное нормальное. Причем метод Марсальи предпочтительнее, т.к. там используется одна трансцендентальная функция $\ln$ на 2 величины вместо тяжелых для вычисления синуса и косинуса. Так из рандома дефолтного получится рандом нормальный, если нам вдруг он понадобится

### Доказательство

Скорее рассуждения насчет доказательства ну да ладно. Формулы при желании на них натянуть можно.

$\upsilon_1$ и $\upsilon_2$ - это те же $u_1$ и $u_2$, только раскиданные не по $[0,1]$, а по $[-1,1]$. А $s$ - это квадрат длины вектора, заданного этими точками. Отсечка в $s\leq 1$ нужна для того, чтобы обеспечить радиальную симметрию области отбора и соответственно не зависеть от $\varphi$

Далее мы наблюдаем крайне странную вещь. Точки внутри единичного круга распределены равномерно, а вот с $s$ история совсем не такая. 

```
это только середина Ксюшиного конспекта............
```

~~проводя вычисления~~ интуитивно понимаем, что точки внутри круга раскиданы равномерно, а значит:

$$
P_{\upsilon_1, \upsilon_2}(x, y) = \frac{1}{\pi}; \quad x^2+y^2 \leq 1
$$

Переходя из декартовых координат в полярные, имеем:

$$
\upsilon_1 = R\cos\Phi \quad \upsilon_2 = R\sin\Phi \quad s = R^2
$$

$$
P_{R, \Phi}(r,\varphi) = P_{\upsilon_1, \upsilon_2}(r\cos\varphi, r\sin\varphi) \cdot |\ \mathfrak{J}\ | = \frac{r}{\pi}
$$

Нашли совместную плотность, поехали искать маргинальные

$$
P_R(r) = \int_{0}^{2\pi} \frac{r}{\pi}d\varphi = 2r \quad \Rightarrow \quad F_R(r) = r^2
$$

$$
F_{R^2}(r) = F_R(\sqrt{r}) = r \quad \Rightarrow \quad P_{R^2}(r) = 1
$$

То есть $R$ имеет линейное распределение, а $R^2 = s$ - равномерное на $[0,1]$. 

Разберемся теперь с $\Phi$:

$$
P_\Phi(\varphi) = \int_0^1 \frac{r}{\pi} dr = \frac{1}{2\pi} \quad \Rightarrow \quad \Phi \sim \mathfrak{U}[0,2\pi]
$$

$\Phi$ распределена равномерно, но напрямую мы её использовать не можем, т.к. по условию мы работаем с декартовыми координатами. Сделаем...

$$
\cos\Phi = \frac{\upsilon_1}{\sqrt{s}} \quad \sin\Phi = \frac{\upsilon_2}{\sqrt{s}}
$$

```
Если вы по инерции ожидали какого-то замудреного смысла в вышеизложенном, то нет, это просто отношения катетов к гипотенузе (s это по определению квадрат длины вектора, а upsilon_1 и upsilon_2 - его декартовы координаты)
```

... и подставим все это месиво в формулы, доставшиеся нам по наследству из Бокса-Мюллера, а именно:

$$
X = \sqrt{-2\ln U_1} \cdot \cos(2\pi U_2)
$$

$$
Y = \sqrt{-2\ln U_1} \cdot \sin(2\pi U_2)
$$

Проворачиваем 3 замены, а именно:

1. Меняем букву $U_1$ на $s$, потому что они обе распределены по $\mathfrak{U}(0,1)$, а значит так сделать можно
2. Не раскладываем $\Phi$ на $2\pi U_2$, а вместо этого подставляем косинус и синус из тригонометрии 7 класса общеобразовательной школы
3. Переименовываем результирующие величины

И вот наконец-то после нескольких суток моих страданий получаем искомые 

$$
x_1 = \sqrt{-2\ln s} \cdot \frac{\upsilon_1}{\sqrt{s}}
$$

$$
x_2 = \sqrt{-2\ln s} \cdot \frac{\upsilon_2}{\sqrt{s}}
$$

ЧиТД вашу ж налево. 2 часа ночи. Завтра с утра на скрипку. Метод Неймана.

# Метод Неймана

И метод отбора. Объединю их, потому что см. краткое содержание

По этой теме я нашла вот [такую](https://studfile.net/preview/6339322/page:8/) вот штуку. Там много буков и в целом вроде бы по теме, но все равно как-то не понятно. Из этого всего я уяснила только то, что через этот метод можно получить распределение, которое напрямую сгенерить тяжело или невозможно.

## Как это было на лекции
Пусть имеется с.в. $X$ с плотностью распределения $f(x)$ на $(a,b)$. Обозначим $M=\underset{a\leq x\leq b}{\max}f(x)$. И определим следующий алгоритм:

> 1. Сгенерируем $u_1, u_2$ по $\mathfrak{U}[0,1]$
> 2. Посчитаем 
> $$x_1 = a + (b-a) u_1$$
> $$x_2 = M \cdot u_2$$
> 3. Если $x_2 \leq f(x_1)$, то $x_1$ берем в качестве реализации искомой с.в. Иначе не берем и генерируем заново (возврат на шаг 1)

## Как это понять на пальцах без формул краткое содержание читать онлайн бесплатно
*Дежавю? Нет, имитационка*

Вообще вот эта хрень это не что иное, как частный случай метода отбора (в некоторых источниках он так и называется - Von Neumann Rejection Sampling и просто Rejection Sampling)

В чем прикол метода отбора, спросите вы? А в том, что мы имеем 3 вещи:

1. Требуемое распределение $f(x)$, которое сгенерить сложно/долго/невозможно
2. Какое-то иное распределение $g(x)$, которое генерить легко (например, равномерное, как в частном случае)
3. Некоторую константу $M$, которая как бы "поднимет" график $g(x)$ так, чтобы он полностью закрыл $f(x)$

И с помощью такого сетапа мы тыкаем пальцем $(x_0, y_0)$ в этот график так, чтобы:

- $x_0 \sim g(x)$
- $y_0 = u\cdot Mg(x_0)$, где $u \sim \mathfrak{U}(0,1)$

Таким образом получается значение, сгенерированное по $g$ с равномерно рандомно выбранной высотой (растянули $\mathfrak{U}(0,1)$ на значение $Mg(x_0)$). Если такой тык в график попал под $f(x)$, то значение идет в финальную выборку, иначе отсекается. Доказывать не буду.

```
Первая часть этой дряни закончилась. Обсидиану плохо, мне тоже
```