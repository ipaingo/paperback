продолжение раздела MCMC

были базовые алгоритмы метрополиса гастингса

задача в том что есть сложное распределение, нормировочная константа неизвестна. требуется вычислить характеристики например мат ожидание

основная идея: генерировать траекторию марковского процесса и считать среднее по траектории

разница что x_i - реализация м п а не выборка

можно когда у м п предельное распределение совпадает с требуемым

слайд 102

генерировать траекторию м п по схеме

вспомогательное распределение. (proposal distribution) предложенное распределение

генерируем кандидата на новое значение $y \sim q(y|_x)$

кандидат принимается с вероятностью а

нормировочная константа в отношении сокращается поэтому не нужно знать распределение до константы

способы выбора распредения путем генерации вектора направления и шага с помощью которого идем в направдении

все частные случаи алгоритма м-г

# схема гиббса (gibbs sampling)

фиксируем x_i, предполагаем что все остальное зафиксировано, считаем полное условное?? и еще что-то считаем

[демка](https://chi-feng.github.io/mcmc-demo/app.html)

окружность потому что ковариация 0, центр мат ожидание, разброс регулируется матрицей ковариаций. стрелка покзывает куда переместиться. стрелка если красная, то состояние не принимается, если зеленая то принимается

## Вариации

MALA - Metropolis Adjusted Langevin Algorithm

стохастическое дифференциальное уравнение

типа дифференциальных, только которое считается не по разностной схеме как из ЧМ (из простейших схема эйлера), а для таких схема эйлера-маруйяма

следующее = предыдущее плюс полшага на градиент логарифма плотности?? плюс приращении. приращение у гауссовского процесса распределено нормально с 0 средним и единичной матрицей ковариации. все приращения независимы друг от друга

в презентахе логарифмы без основания натуральные

набула (перевернутая дельта) - градиент

смысл этой хрени - градиентный подъем. сойдется в максимуме распределения $p(x)$

максимум у $p(x)$ и $\ln p(x)$ достигается в одной точке. почему?

корень шага это шум. искажаем направление чтобы не застрять в локальных экстремумах

внизу на фотке вспомогательное распределение

плотность многомерной с в с нужными параметрами с точностью до константы

в демке МАЛА

концентрические окружности потому что нормальное изотропное распределение

пусть распределение в виде тонкого кольца тогда изначально большой шаг переходит из малой вероятности в тоже малую вероятность. выход? уменьшить шаг

как же все понятно хоспаде блин

размер шага может меняться уменьшаться по какомй-то определенному правилу

если целевое распределение представлено вивиде произведения большого кол-ва других распределений, может возникнуть проблема расчета градиента - большое кол-во слагаемых

тогда используется стохастических градиентный спуск - оценка градиента а не сам градиент

оценивается градиент по слайду 17 из презентахи которой у нас нет))))

не будет в экзе и в лабах но тем не менее

гамильтонов (гибридный) монте карло

на слайде опечатка второе равно должно быть умножением $Z$ - неизвестная нормировочная константа

что-то со статистической физикой если будет интересно посмотрю

ввести вспомогательную с в (по физически импульс) $K(y)$ - кинетическая энергия

и что дальше?? ЗСЭ и ЗСИ?

рассмотреть совместное распределение полной энергии?

полная производная по времени = 0
на кривой какой-то h(x,y) принимает постоянное значение

в демке рисуются гистограммы - маргинальные распределения по x и y

# эл-ты байесовской статистики

слайд 119

одно из основных приложений МСМС - байесовская статистика

формула байеса из тервера

парадокс кого-то там

игра 3 шкатулки, приз только в одной. ведущий открывает одну пустую шкатулку и предлагает поменять выбор

сложно объясняет но максимально просто понять

постановка задачи

пусть y - некоторая наблюдаемая величина размерности n, x - скрытая ненаблюдаемая

предполагается, что x и y - реализации с. в. X Y

по результатам измерения Y найти оценки для X

это возможно когда они связаны друг с другом

$$Y=f(X,E)$$ 

где E - ошибка измерения

модель порождает условное распределение $Y|_X$

пример: $y=x+\varepsilon\ e\sim N(0, \sigma^2)$

$y|_x \sim n(x, \sigma^2)$

пусть зафиксирован y. тогда по условному распределению совместное на знаменатель которфй не трогаем

числитель = х на условное

обзовем 

$x|_y$ - скрытое (апостериорное) распределение

$y|_x$ - ф-я правдоподобия

$x$ - априорное распределение

знаменатель - обоснованность (evidence)

апостериорное распределение зависит от априорного с точностью до константы

п(у) сложно найти. почему?

$$
\pi(y) = \int_{}^{} \pi(y|_x)\pi(x)\ dx
$$

неизвестна нормировочная константа но для МСМС она и не нужна

смотрим

bayesian inference 

подразумевается поиск апостериорного распределения выражение либо оценка его числовых характеристик (самое популярное мода и мат ожидание)

мода - оценка максимума апостериорного распределения. нормировочная константа не влияет. удобнее переходить к логарифму. почему??

если мат ожидание, то будет условное ожидание 

$$\mathbb{E}(X|_Y) = \int x\pi(x|_y)\ dx$$

задается функция потери я начало прослушала $L(x,y)$, берется мат ожидание относительно апостерииорного, минимизируется это выражение. минимизация ошибки??

такое мат ожидание наз-ся байесовским риском. если в кач-ве потери выбрано СКО, то х с крышкой это условное мат ожидание

в скалярном случае если есть мат ожидание и квадрат разности

$$\mathbb{E}[(x-a)^2] \rightarrow \min$$

будет минимальным если A - мат ожидание. получится дисперсия. так же для многомерного случай

на слайде 121 2 вида оценок. что брать мат ожидание или моду? могут не совпать. зависит от задачи??

картинки с доски. что праводподобнее или что легче

максимум ищется оптимизацией 

для МСМС интересно мат ожидание

если есть интеграл

$$\int x\pi(x|_y)\ dx$$

то его можно оценить

если есть выборка из апостериорного, хорошо

$$\int x\pi(x|_y)\ dx ≈ {1 \over n} \sum_{i=1}^n x_i$$

игрушечные примеры не буду записывать. слайд 123

## Обратные задачи математической физики

интерпретация скорее биологическая (что не лучше)

уравнение Фишера-Колмогорова-Петровского-Пискунова

слайд 133

возможно уравнение теплопроводности я потом посмотрю эту хрень звучит интересно. или уравнение диффузии. распределение популяции организмов вдоль пространственной переменной $x$ и с течением времени $t$. так численность в точке $x$ в момент $t$

$x$ одномерный что на практике не очень, но вполне себе применимо для например планктона, численность которого зависит в бОльшей степени от глубины

### Применение байесовсих чего-то там в машинном обучении

правильно ли мы друг друга понимаем сказал Лукашенко

яб ответила, что нихрена мы не понимаем, но не буду. пойму потом когда-нибудь

стандартная помтановка задачи обучения

набор данных Дэ красивое лмао

задача регрессии чего-то там куда-то там

любая модель МО - функция от экземпляра входящей выборки и набора параметров. вектор какой-то размерности 

постановка задачи - предсказать $\hat y_i$ на основании $x_i$

задается ф-я потери $1/n \sum l(y_i, \hat y_i) \rightarrow \min_{\Theta}$

часто работают с минимумом квадратичной ошибки (как дисперсия наверное). получается МНК

куда-то тащим МНК, сил уже нет осознавать да и Ден написал впервые за вечность. что-то со скалярным произведением и почему-то параметры в степень возводятся. как.....

если это будет на экзамене или среди теории на лабы, то простите меня пж. Была порекомендована к прочтению книжка Кристофера Бишопа "Pattern Recognition and Machine Learning"