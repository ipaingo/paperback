# Методы понижения дисперсии оценки

Или как обойтись меньшим объемом выборки, но сделать более точную оценку на её основе

В иностранной литературе подобную инфу можно найти по ключевым словам Variance Reduction Methods

## Предисловие

В конце предыдущих 20 страниц моего страдания была небольшая пометка о том, что с увеличением кол-ва отражений растет и шум в картинке. И в этом разделе будет примерно то, с помощью чего можно эти шумы немножко подубрать.

Возвращаясь к первой лекции, имеем формулу для доверительного интервала

$$
\mathbb{P}\ \left( \overline{x} - \frac{z_\gamma\sigma}{\sqrt{n}} < a < \overline{x} + \frac{z_\gamma\sigma}{\sqrt{n}} \right) ≈ \gamma
$$

где $z_\gamma = \Phi^{-1}(\frac{\gamma}{2})$.

Как мы видим (для нормального распределения, получено магией матанализа и статистики), в оценке доверительного интервала присутствует СКО, а также объем выборки. Поскольку первый в числителе, а второй в знаменателе, один разброс повышает, второй понижает. 

### Апдейт

Кстати, настоятельно рекомендую пораскапывать вот ту статейку из вики, вероятность преисполниться повысится.

[Англовики](https://en.wikipedia.org/wiki/Variance_reduction#Crude_Monte_Carlo_simulation) предлагает гораздо более понятное объяснение того, почему же дисперсия так сильно влияет на точность оценки. Если кратко, то согласно ЦПТ (если оцениваемый параметр - выборочное среднее, например):

$$\overline{z}={1\over n}\sum_{i=1}^n z_i \sim \mathbb{N}\Big(\overline{z}, {\sigma\over\sqrt{n}}\Big)$$

Ессесна мы хотим, чтобы отклонение было как можно меньше (читаем - стало примерно 0). И вот вам и СКО в числителе, и корень объема выборки в знаменателе. Как будто бы чего-то не хватает, но чего?..

Энивей, логично предположить, что можно увеличить выборку (что так-то иногда возможно ценой увеличения вычислительной нагрузки), а можно уменьшить дисперсию.

Как? Смотрим.

```
*агрессивно включает заставку из спокойной ночи малыши потому что нас теперь уже спасут только мультики и розовые очки*
```

## Лирическое отступление

Введем понятие ==относительной ошибки== - она же коэф-т вариации оценки. Это отношение СКО к мат.ожиданию. Сделано это для того, чтобы повысить чувствительность для близких к нулю значений. Зачем? Сама задумываюсь.

$$
RE(\overline{x}) = {\sqrt{\mathbb{D}\overline{x}} \over \mathbb{E}\overline{x}}
$$

где $\overline{x}$ - некоторая оценка параметра $a$

В качестве примера были рассмотрены 2 выборки из 3х элементов: $\{1,2,3\}$ и $\{10^{-2}, 10^{-6}, 10^{-8}\}$. Пример иллюстрирует данный факт? Да. Но зачем?

# Метод дополнительных (дополняющих) случайных величин

(antithetic variables)

Идея такого метода звучит как магия вне Хогвартса, и при должной фантазии таковой и является. По определению выборка - последовательность независимых реализаций с.в.. Разложим исходную с.в. на комбинацию (например среднее) других, вводя зависимость.

Так как дисперсия считается от квадрата, то и раскрывается она тоже по квадрату:

$$
\mathbb{D}\Big[{x_1+x_2 \over 2}\Big] = {1\over 4}\Big(\mathbb{D}\big[x_1\big] + \mathbb{D}\big[x_2\big] + 2cov(x_1, x_2)\Big)
$$

> [!important] Важный нюанс
> Замену надо подобрать так, чтобы ковариация была отрицательной, тогда дисперсия уменьшится. Иначе увеличится.

Если применяется метод обратной функции, то замену можно произвести следующим образом:

$$
x_1 = F^{-1}(u) \quad x_2 = F^{-1}(1-u)
$$

В таком случае обе с.в. будут зависимы от $u$ с ковариацией $-{1 \over 12}$

Имеем следующее утверждение: если $f(x_1,...,x_n)$ монотонна по каждому аргументу, то:

$$
cov\Big[f(u_1,...,u_n), f(1-u_1,...,1-u_n)\Big] \leq 0
$$

где $u_1,...,u_n \sim \mathfrak{U}(0,1)$

Почему? Возможно докажу позже. Но если мы пока что примем это как факт, то сможем сделать вывод о том, что при такой схеме дисперсия гарантированно не увеличится. В лучшем случае она уменьшится, в худшем - останется как была.

Из этого формируем ещё одно весьма неочевидное но легко осознаваемое следствие - метод применим для монотонных функций. Иначе есть риск положительной ковариации, а как следствие - увеличения дисперсии.

# Метод контрольных случайных величин

(control variate method)

_Маленькое примечание:_ мю в записях с лекций и мю в англовики это две разных мю. Здесь буду придерживаться Лукашенковского формата, но мало ли кто полезет смотреть в интернет и увидит там мю совершенно в другом контексте.

### Жесть

Итак, правила игры следующие: имеем оцениваемый $\mathbb{E}(X)$, и некоторый $Y$, такой что $\mathbb{Y} = \mu$. Нафиг нам этот игрек нужен? А это для трюка из разряда добавить и вычесть - результат не меняется, зато добавляется красивое/полезное/прикольное/а-почему-бы-и-нет слагаемое:

$$
Z = X + c(Y - \mu)
$$

Поскольку здесь фигурирует, хоть и умноженное на какую-то константу, но всё же $Y-\mu$, то мат.ожидание такого зверя останется таким же в теории и ну примерно таким же на практике. Плюс-минус метр, как говорили наши парни, играя в бильярд. А вот дисперсия:

$$
\mathbb{D}[Z] = \mathbb{D}\Big[X+c(Y-\mu)\Big] = \mathbb{D}X + c^2\mathbb{D}Y + 2c\ cov(X,Y)
$$

Или чуть более читабельно (имхо):

$$
\mu_2(Y)\cdot c^2 + \mu_{1,1}\cdot 2c + \mu_2(X)
$$

**Q:** ЧЗХ??? На кой тут вообще $c$???

**A:** Терпение, ща всё будет. Ну мало ли кого заинтересовало нафиг вообще сюда эта злосчастная константа прилеплена

Так вот, ladies and gentlemen and everyone in-between. Если на время отвлечься от статистики и назвать вещи школьными именами, то что дисперсии, что ковариация, на практике станут вполне себе известными циферками, иначе - константами, а та самая $c$ - переменной. _How the turns tabled однако_

Решим задачу оптимизации. Минимизируем дисперсию, двигая единственную неизвестную букву в получившемся бедламе.

### Как уменьшить букву (пособие для забывших матанализ)

> [!check] Дано
> - ✨Буква✨
> - Уравнение этой буквы (в нашем случае такое вот)
> 
> $$
\mu_2(Y)\cdot c^2 + \mu_{1,1}\cdot 2c + \mu_2(X)
> $$

> [!question] Найти
> Минимум буквы

Минимум значит экстремум, а значит что? Правильно, матанализ 1 курс, тема производные. Функция в нашем случае вышла квадратичная, график, сюрприз-сюрприз, парабола, минимум существует всегда, так как при $c^2$ стоит дисперсия - величина по умолчанию неотрицательная. Путем неприлично простых рассуждений получаем, что

$$
c^* = -\frac{\mu_{1,1}}{\mu_2(Y)} = -\frac{cov(X,Y)}{\mathbb{D}(Y)}
$$

Что к тому же является одним из коэффициентов линейной регрессии так-то. Как это связано? Да хз, видимо в этом что-то есть.

### Жесть (продолжение)

Подставляя полученный минимум в $\mathbb{D}Z$ и считая ${\mathbb{D}Z \over \mathbb{D}X}$, получаем, что

$$
{\mathbb{D}Z \over \mathbb{D}X} = 1 - \text{corr}^2(X,Y)
$$

Что это всё значит? А то, что такой метод работает всегда, в отличие от предыдущего, где мы могли упереться в положительную ковариацию и всё попортить. В процессе перечитывания лекций может показаться, что это отношение взято с потолка, и по сути так оно и есть. Но сакральный смысл этого бреда никто не отменял, а поэтому всмотримся в то, что оно по-любому попадает между 0 и 1 и сделаем весьма приятный для нас вывод. _Полученная дисперсия всегда окажется не больше исходной_