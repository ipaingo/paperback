# Моделирование многомерных с.в.
Пусть нам потребовалось сгенерировать случайный вектор $X = (X_1, X_2, ..., X_n)$, имеющий плотность распределения $f(x_1, x_2, ..., x_n)$. Для этого вспомним, что 

$$
\begin{aligned}
& f(x_1,...,x_n) = f_1(x_1)\cdot f_2(x_2|_{x_1}) \cdot\ ...\\
& \cdot f_n(x_n|_{x_1,x_2,...,x_{n-1}})
\end{aligned}
$$

из чего выводится

$$
f_i(x_i|_{x_1^*...x_{i-1}^*}) = \frac{f(x_1,...,x_n)}{f_{i-1}(x_1,...,x_{i-1})}
$$

согласно свойству условного распределения (см. 1 тервер)

1. Реализуется выборочное значение $X_1^*$ компоненты $X_1$ согласно плотности $f_1(x_1) = \int...\int f(x_1, x_2, ..., x_n)dx_2dx_3...dx_n$ (проинтегрировали исходную по всем другим переменным). Как это делать было рассмотрено одним файлом ранее.
2. Реализуется $X_2^*$ компоненты $X_2$ по плотности $f_2(x_2|_{x_1^*})$
3. Реализуются все остальные компоненты по такому же принципу

**Q: Почему не взять просто компоненты без условностей?**

**A:** Да потому что они так или иначе друг на друга влияют. Выбрав одну компоненту, мы задаем плотность распределения всех остальных, которые мы и разворачиваем по одной.

**Q: Почему бы просто не остановиться на этом?**

**A:** Если внезапно потребуется что-то большое, то $n-1$-интегралы в количестве $n$ штук будут считаться до посинения

_Тут дальше идет какой-то пример, может разберу потом_

## Метод Неймана
*Дежавю? Нет, имитационка*

В общем и целом суть та же, что и в одномерном случае, только мы растягиваем эту катавасию на несколько координат. Если переместимся в 3д, увидим поверхность, ограниченную сверху куском плоскости, параллельной $O_{xy}$. Кидаем на этот прямоугольник (куб/тессеракт/...) точку по равномерному распределению, высоту выбираем по $\mathfrak{U}(0,M)$, если попала под исходную поверхность, берем, иначе пропускаем. Картинки не будет, ибо я абсолютно хз, как это представить адекватно одним скрином. Если у 3b1b будет объясняшка на эту тему, я скину

### А теперь умными буквами

Пусть надо сгенерировать случайный вектор $X$, $i$-я компонента которого распределена каким-либо образом на $[a_i, b_i]$. Пусть также известна совместная плотность $f(x_1, ..., x_n)$ и $M=\max f(x_1, ..., x_n)$. Тогда:

1. Генерируются случайные числа $R_1, ..., R_n$, равномерно распределенные на $[a_1, b_1], ..., [a_n, b_n]$, а также (выделю отдельно, так как это прям _важно_ важно) $R_{n+1} \sim \mathfrak{U}(0,M)$
2. Если $R_{n+1} \leq f(R_1, ..., R_n)$, то в качестве реализации вектора берется $(R_1, ..., R_n)$, иначе генерируем снова

# Многомерное нормальное распределение

## Как это было на лекции
Пусть требуется сгенерировать реализацию нормально распределенного центрированного (мат. ожидания всех компонент = 0) вектора $X = (X_1, ..., X_n)^T$ с заданной матрицей ковариаций (на доске была $\Sigma(\Sigma_{ij})$, на слайдах $\Gamma(\gamma_{ij})$, запишу как на слайдах)

$$
\gamma_{ij} = \begin{cases}
\mathbb{D}X_i, & \quad i=j \\
cov(X_i, X_j) & \quad i \neq j
\end{cases}
$$

### Разложение Холецкого

$$
\Gamma = CC^T
$$

где $C$ - нижняя треугольная матрица с $C_{ii}>0$

Пусть сгенерирован $Y=(Y_1, ..., Y_n)^T$ - случайный вектор, состоящий из независимых стандартных нормальных с.в. Можно доказать, что вектор $Z = C \cdot Y$ распределен так же, как и искомый $X$

$$
\mathbb{E}[ZZ^T] = \mathbb{E}[CYY^TC^T] = CE[YY^T]C^T = CC^T = \Gamma
$$

Примечания к этой выкладке:

$$
ZZ^T = \begin{pmatrix}
z_1\\z_2\\\vdots\\z_n
\end{pmatrix}
\begin{pmatrix}
z_1 & z_2 & \cdots & z_n
\end{pmatrix} = 
\begin{pmatrix}
z_1z_1 & z_1z_2 & \cdots & z_1z_n \\
z_2z_1 & z_2z_2 & \cdots & z_2z_n \\
\vdots & \vdots & \ddots & \vdots \\
z_nz_1 & z_nz_2 & \cdots & z_nz_n
\end{pmatrix}
$$

$$
\mathbb{E}(z_iz_j) = \mathbb{E}[z_i-\mathbb{E}(z_i)\cdot z_j - \mathbb{E}(z_j)] = cov(z_i, z_j)
$$

Вот для этой ^^^ записи нам и нужны нулевые мат. ожидания. Если бы $\mathbb{E}(z_i)$ были не 0, то мы бы не смогли перевести начальный момент в центральный просто так взяв и вычеркнув разность. Кто не верит - раздел "Числовые характеристики многомерного случайного вектора" в моем конспекте по 1 терверу, секция про начальные и центральные моменты.

## Как это понять на пальцах без формул краткое содержание читать онлайн бесплатно

Посмотрим правде в глаза и признаем, что из предыдущих выкладок нам стало понятно ровно ничего. 

Так вот, _поясняю_


> [!success] Дано:
> - Центрированное нормальное распределение случайного вектора (мат. ожидания всех компонент нулевые, _а вот ковариации не 0 (есть зависимость компонент)_)
> - Собсна матрица ковариаций этих компонент
> - Стандартизованная нормальная генерация случайных величин (полученная, например, из прошлой лекции)

> [!question] Найти
> Как из сгенерированного вектора с независимыми компонентами получить нормальный с нужной зависимостью компонент

### Матрица ковариаций

$$
\gamma_{ij} =
\mu_{1,1}(i,j) = cov(x_i, x_j)
$$

Что это такое? Смешанный момент порядка 1,1 (определение в тервере). Иначе говоря, мат. ожидание от смещений компонент. Для разных компонент это ковариация, для одной $(i=j)$ - дисперсия

> Интересный факт: Если ненулевые только дисперсии (как например в случае имеющейся генерации), компоненты независимы. Иначе наблюдаем некоторую зависимость. 

### Разложение Холецкого

Зачем вообще раскладывать на нижнюю треугольную и верхнюю треугольную матрицу? Да всё просто - упростить вычисления. В геометрический интерпретации этого метода мы кидаем точку в ортонормальном базисе, а потом его сдвигаем так, чтобы наблюдалась нужная нам корреляция м/у новыми базисными векторами.

Такое преобразование работает только тогда, когда матрица ковариаций положительно определенная, то есть все собственные значения положительны. Что это такое спросите вы? А я вас отправлю в конспекты по линалу, потому что не помню от слова совсем. Но это важно.

Таким образом 

$$
\Gamma = CC^T = 
\begin{pmatrix}
c_{11} & 0 & \cdots & 0 \\
c_{12} & c_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
c_{1n} & c_{2n} & \cdots & c_{nn}
\end{pmatrix}
\begin{pmatrix}
c_{11} & c_{12} & \cdots & c_{1n} \\
0 & c_{22} & \cdots & c_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & c_{nn}
\end{pmatrix}
$$

Отсюда нетрудно понять, что по правилу умножения матриц:

$$
\begin{cases}
\gamma_{11} = {c_{11}}^2 \\
\gamma_{12} = c_{11} \cdot c_{12} \\
\cdots \\
\gamma_{1n} = c_{11} \cdot c_{1n} \\
\cdots \\
\gamma_{21} = c_{12} \cdot c_{11} \\
\gamma_{22} = {c_{12}}^2 + {c_{22}}^2 \\
\gamma_{23} = c_{12} \cdot c_{13} + c_{22}\cdot c_{23} \\
\cdots \\ 
\gamma_{nn} = \underset{i=1}{\overset{n}{\sum}} {c_{in}}^2
\end{cases}
$$

Гаммы нам известны, $c$ нет, но мы их легко посчитаем друг за другом, раскрывая гамму построчно или постолбцово. Никакой магии - терпение, линал и стопка коньяка

```
постолбцово...... прикольное слово однако
```

_Примечание: этот способ разложения матрицы ковариаций не единственный, но один из наименее затратных в вычислительном плане_

### Что дальше?

Посчитали мы новую матрицу, а потом просто банально делаем так:

$$
Z = CY
$$

где $Y$ - сгенерированный вектор, а $Z$ - искомый

$$
\begin{pmatrix}
z_1 \\ z_2 \\ \vdots \\ z_n
\end{pmatrix} = 
\begin{pmatrix}
c_{11} & 0 & \cdots & 0 \\
c_{12} & c_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
c_{1n} & c_{2n} & \cdots & c_{nn}
\end{pmatrix}
\begin{pmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
$$

Почему так можно? Вспомним про то, что все мат. ожидания 0, а значит по формуле смешанного момента _(да и центрального тоже так-то)_

$$
\mu_{11}(ij) = \mathbb{E}\Big[\big(y_i - \mathbb{E}[y_i]\big)\big(y_j - \mathbb{E}[y_j]\big)\Big] = \mathbb{E}\big[y_iy_j\big]
$$

Так же и для $z_iz_j$ А значит можно пойти через...

$$
ZZ^T = \begin{pmatrix}
{z_1}^2 & z_1z_2 & \cdots & z_1z_n \\
z_2z_1 & {z_2}^2 & \cdots & z_2z_n \\
\vdots & \vdots & \ddots & \vdots \\
z_nz_1 & z_nz_2 & \cdots & {z_n}^2
\end{pmatrix}
$$

... и посчитать смешанный момент для каждого из этих значений

Раскрываем $Z$ и $Z^T$

$$
\mathbb{E}[ZZ^T] = \mathbb{E}[CYY^TC^T]
$$

Выносим $C$ и $C^T$ за мат. ожидание (так можно не всегда, но тут можно)

$$
\mathbb{E}[CYY^TC^T] = C\ \mathbb{E}[YY^T]\ C^T
$$

А поскольку дисперсии _стандартизованных нормальных величин_ все = 1, а ковариации = 0, то матрица ковариаций такого вектора - единичная

$$
C\ \mathbb{E}[YY^T]\ C^T = CIC^T = CC^T = \Gamma
$$

А значит искомый вектор соответствует желаемому распределению

# Моделирование случайных процессов

Пусть задан с.п. $\big(X_t, t \in \mathbb{T}\big)$. Реализовать такой с.п. означает реализовать случайный вектор $(X_{t_1}, X_{t_2}, ..., X_{t_n})$, где $t_1, ..., t_n \in \mathbb{T}$

С.п. явл-ся ==гауссовским==, если 

$$
\forall t_1,...,t_n \in \mathbb{T} \quad (X_{t_1}, ..., X_{t_n}) \sim \mathbb{N}
$$

если с любыми $t$ искомый вектор имеет многомерное нормальное распределение. Распределение такого процесса полностью описывается ковариационной функцией

$$
\Gamma(s,t) = cov(X_s, X_t)
$$

Дальше идут какие-то пояснения на тему того, что эта задача таким образом сводится к предыдущей. И примеры, но их не будет.

```
это что, всё??? ну ладно, кто извлек из этих записей хоть какую-то пользу, я за вас рада
```