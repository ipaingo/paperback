==$\aleph$== *==линейная регрессия==* — это регрессионный анализ для подбора параметров в $y = f(x)$, в котором целевой функционал будет минимальным. один из видов целевого функционала метода наименьших квадратов, т. е. частный случай МНК, в котором $f(x)$ — линейная функция.

на оси $X$ располагается переменная на основе которой осуществляется предсказание.
на оси $Y$ располагается переменная, которую нужно предсказать.

$$y_i = a \cdot x_i + b$$

$a$ — угловой коэффициент, характеризующий наклон прямой. показывает среднее изменение функции с изменением параметра на единицу.
$b$ — константа, определяющая точку пересечения с осью $Y$.

$a = r \cdot \dfrac{\sigma_x}{\sigma_y}$, где $r$ — коэффициент корреляции Пирсона, $\sigma_x,\ \sigma_y$ — стандартные отклонения для $X$ и $Y$.
$b = \bar y - a \cdot \bar x$

чтобы определить наилучшую линию регрессии используется *==метод наименьших квадратов==*.

## 1. $y = ax+b$

нужно найти такие параметры $a$ и $b$, что:
$$l(x_1,\ ...,\ x_n,\ a,\ b) = \underset{i=1}{\overset{n}{\sum}} {(y_i - a\cdot x_i - b)}^2 \underset{a,\ b}{\to} \min$$

чтобы найти минимум функции, нужно вычислить частные производные по каждому из параметров $a$ и $b$ и приравнять к нулю:

$$\begin{cases} \dfrac{\partial l}{\partial a} = -2 \sum (y_i - ax_i - b) \cdot x_i = 0 & \quad (1) \\ \dfrac{\partial l}{\partial b} = -2 \sum(y_i - ax_i - b) = 0 & \quad (2)\end{cases}$$

$\dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} y_i - a \dfrac{1}{n}\underset{i=1}{\overset{n}{\sum}}x_i - b = 0$
$\bar y - a \bar x - b = 0$
$b = \bar y - a \bar x$

$(1)$ раскроем скобки и поделим на $-\dfrac{2}{n}$.

$$2 \underset{i=1}{\overset{n}{\sum}} y_i x_i - 2 \underset{i=1}{\overset{n}{\sum}} a x_i -2 \underset{i=1}{\overset{n}{\sum}} b x_i = 0$$

$$\dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} y_i x _i - a \dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} x_i^2 - b \dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} x_i = 0$$

$$\dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} y_i x_i - a \dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} x_i^2 - (\bar y - a \bar x) \bar x = 0$$

$$\underset{m_{11}\ cov \text{ смеш. момент}}{\underbrace{\dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} y_i x_i - \bar y \bar x }} - \underset{m_{2x} \text{ выб. дисперсия }}{\underbrace{a \dfrac{1}{n} \underset{i=1}{\overset{n}{\sum}} x_i^2 - a \bar x^2}} = 0$$

$$m_{11} - a m_{2x} = 0 \Rightarrow a = \dfrac{m_{11}}{m_{2x}}$$

$\rho = \dfrac{m_{11}}{\sqrt{m_{2x} \cdot m_{2y}}}$ чтобы получить $a$, нужно домножить на $\dfrac{\sqrt{m_{2y}}}{\sqrt{m_{2x}}}$.

$\rho \cdot \dfrac{\sqrt{m_{2y}}}{\sqrt{m_{2x}}} = \rho \dfrac{\sigma_y}{\sigma_x} = a$

$$y = \underset{a}{\underbrace{\rho \dfrac{\sigma_y}{\sigma_x}}}x + \underset{b}{\underbrace{\bar y - \rho \dfrac{\sigma_y}{\sigma_x} \bar x}}$$

$$\textcolor{ProcessBlue}{\large{y - \bar y = \rho \dfrac{\sigma_y}{\sigma_x} (x - \bar x)}}$$



## 2. $x=a_1y + b_1$

```
господи помилуй.
```

$x$ — функция от $y$.

$$\textcolor{ProcessBlue}{\large{x - \bar x = \rho \dfrac{\sigma_x}{\sigma_y} (y - \bar y)}}$$

## че там еще.

точка пересечения графиков $y$ и $x$ имеет координаты их мат. ожиданий.

$y = \dfrac{1}{\rho} \dfrac{\sigma_y}{\sigma_x} (x - \bar x) + \bar y$

если величины линейно зависимы, то они совпадают. если линейной зависимости нет, то отличаются невязки.
```
(курс численных методов: невязка это типа абсолютная погрешность.)
```

сумма квадратов невязок минимальна (в этом суть метода). вопрос: какой график строить, $x$ на $y$ или $y$ на $x$?

$l = \underset{i=1}{\overset{n}{\sum}} {\left(y_i - \rho\dfrac{\sigma_y}{\sigma_x} x_i - \bar y + \rho\dfrac{\sigma_y}{\sigma_x} \bar x\right)}^2$ $=$ $\underset{i=1}{\overset{n}{\sum}}{\left((y_i - \bar y) - \rho\dfrac{\sigma_y}{\sigma_x}(x_i - \bar x)\right)}^2$ $=$ $\underset{i=1}{\overset{n}{\sum}} (y_i - \bar y)^2 - 2 \underset{i=1}{\overset{n}{\sum}} (y_i - \bar y) \rho\dfrac{\sigma_y}{\sigma_x}(x_i - \bar x) + \rho^2\dfrac{\sigma_y^2}{\sigma_x^2} \underset{i=1}{\overset{n}{\sum}} {(x-\bar x)}^2$ $=$ $n \cdot m_{2y} - 2 \rho\dfrac{\sigma_y}{\cancel{\sigma_x}} \overset{= \rho \sigma_y \cancel{\sigma_x}}{ \cdot\ m_{11}} + \rho^2\dfrac{\sigma_y^2}{\cancel{\sigma_x^2}} n \cdot \cancel{\sigma_x^2}$ $=$ $n \cdot m_{2y} - 2 \rho^2 \sigma_y^2 + \rho^2 n \sigma_y^2$ $=$ $n \cdot m_{2y} - n m_{2y} \rho^2 = n m_{2y}(1 - \rho^2)$

$l = n \cdot m_{2y} \cdot (1- \rho^2) \Rightarrow$ лучше строить график той величины, у которой меньше дисперсия.